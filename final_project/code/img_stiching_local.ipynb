{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image stiching and localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the frames from a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = Path(\"/home/cv_stu_03/Hierarchical-Localization/project/local_data/video\")\n",
    "images_path = Path(\"/home/cv_stu_03/Hierarchical-Localization/project/local_data/images_from_local\")\n",
    "output_path = Path(\"/home/cv_stu_03/Hierarchical-Localization/project/local_data/image_stitch\")\n",
    "# do extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do stiching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 间隔为，选取需要进行定位的那张照片的前后间隔多少帧的照片来进行 image stiching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reference 是顺序的视频帧，我们需要使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_references = sorted([p.relative_to(images_path).as_posix() for p in (images_path).iterdir()])\n",
    "# references\n",
    "# for i in range(len(references)):\n",
    "#     print(references[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(references, images_path, image_reference, interval):\n",
    "    # references 是上述的得到的图像序列\n",
    "    # images_path 即为上述的 images path\n",
    "    # image_reference 是需要做定位的图片，需要找到前后的 interval 的图像\n",
    "    ret_images = []\n",
    "    index = references.index(image_reference)\n",
    "    if (index-interval) < 0:\n",
    "        return ret_images\n",
    "    elif (index+interval) >= len(references):\n",
    "        return ret_images\n",
    "    \n",
    "    ret_images.append(cv2.imread(str(images_path/references[index - interval])))\n",
    "    ret_images.append(cv2.imread(str(images_path/references[index])))\n",
    "    ret_images.append(cv2.imread(str(images_path/references[index + interval])))\n",
    "    return np.array(ret_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_images = get_images(images_references, images_path, images_references[24], interval)\n",
    "if len(ret_images)==0 :\n",
    "    print(\"no images\")\n",
    "else:\n",
    "    print(ret_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do image stitching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0_rgb = ret_images[0]\n",
    "img1_rgb = ret_images[1]\n",
    "img2_rgb = ret_images[2]\n",
    "\n",
    "print(img0_rgb.shape)\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img0_rgb)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(img1_rgb)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(img2_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img1_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use SIFT to compute the keypoints and desciptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_matches(image0, image1):\n",
    "    SIFT = cv2.SIFT_create()\n",
    "\n",
    "    keys0, descb0 = SIFT.detectAndCompute(image0, None)\n",
    "    keys1, descb1 = SIFT.detectAndCompute(image1, None)\n",
    "\n",
    "    bf = cv2.BFMatcher()\n",
    "\n",
    "    matches = bf.knnMatch(descb0, descb1, 2)\n",
    "\n",
    "    # print(\"the lenth of matches is {}\".format(len(matches)))\n",
    "\n",
    "    good_matches = []\n",
    "\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.6*n.distance:\n",
    "            good_matches.append([m])\n",
    "\n",
    "    print(\"the lenth of good matches is {}\".format(len(good_matches)))\n",
    "\n",
    "    match_result1 = cv2.drawMatchesKnn(image0, keys0, image1, keys1, good_matches, None, flags=2)\n",
    "    # plt.imshow(match_result1)   \n",
    "\n",
    "    mkpts0 = np.zeros((len(good_matches), 2))\n",
    "    mkpts1 = np.zeros((len(good_matches), 2))\n",
    "\n",
    "    for i, match in enumerate(good_matches):\n",
    "        img0_idx = match[0].queryIdx\n",
    "        img1_idx = match[0].trainIdx\n",
    "        \n",
    "        mkpts0[i, :] = (keys0[img0_idx].pt) # pt is the coordinate of the matched points\n",
    "        mkpts1[i, :] = (keys1[img1_idx].pt)\n",
    "\n",
    "    # print(\"the match points in image 1 is {}\".format(mkpts0))\n",
    "    # print(\"the match points in image 2 is {}\".format(mkpts1))\n",
    "    \n",
    "    return mkpts0, mkpts1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the inliers\n",
    "def cal_inliers(sample1, sample2, h, inlier_thr):\n",
    "    sum_inliers = 0\n",
    "    h_reshape = np.zeros((3, 3))\n",
    "    h_reshape[2, 2] = 1\n",
    "    sample1_reshape = np.ones((3, ))\n",
    "    sample2_reshape = np.ones((3, ))\n",
    "    for i in range(sample1.shape[0]):\n",
    "        h_reshape[0:2, :] = h.reshape((2, 3))\n",
    "        sample1_reshape[:2] = sample1[i]\n",
    "        sample2_reshape[:2] = sample2[i]\n",
    "        if np.sum(np.square(h_reshape @ sample1_reshape - sample2_reshape)) < inlier_thr:\n",
    "            sum_inliers += 1\n",
    "\n",
    "    return sum_inliers\n",
    "\n",
    "\n",
    "# implement your own RANSAC\n",
    "def ransac_to_estimate_H(sample1, sample2, K, inlier_thr, M): \n",
    "    H = None\n",
    "    max_inliers = 0\n",
    "    for i in range(M):\n",
    "        rand_matches = [random.randint(0, sample1.shape[0]-1) for _ in range(K)]\n",
    "        \n",
    "        selected_mkpts0 = [sample1[m] for m in rand_matches]\n",
    "        selected_mkpts1 = [sample2[m] for m in rand_matches]\n",
    "\n",
    "        A = np.zeros((6, 6))\n",
    "        b = np.zeros((6, ))\n",
    "        \n",
    "        for i in range(K):\n",
    "            A[2*i,0 :2] = selected_mkpts0[i]\n",
    "            A[2*i, 2] = 1 \n",
    "            A[2*i+1, 3:5] = selected_mkpts0[i]\n",
    "            A[2*i+1, 5] = 1\n",
    "            b[2*i:2*i+2] = selected_mkpts1[i]\n",
    "\n",
    "        h = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "        if cal_inliers(sample1, sample2, h, inlier_thr) > max_inliers:\n",
    "            max_inliers = cal_inliers(sample1, sample2, h, inlier_thr)\n",
    "            H = h\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the stitching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_stitch(image1, image2):\n",
    "    mkpts0, mkpts1 = get_good_matches(image1, image2)\n",
    "    \n",
    "    # transform image1 to stitch on image2\n",
    "    H = ransac_to_estimate_H(mkpts1, mkpts0, 3, 5, 100) \n",
    "\n",
    "    H_optimized = np.zeros((3, 3))\n",
    "    H_optimized[2, 2] = 1\n",
    "    H_optimized[:2, :] = H.reshape((2, 3))\n",
    "\n",
    "    height, width, channels = image1.shape\n",
    "    dsize = (width*2, height)\n",
    "\n",
    "    panorama = cv2.warpPerspective(image2, H_optimized, dsize)\n",
    "\n",
    "    return panorama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do the stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img0_rgb)\n",
    "temp_stitch = image_stitch(img1_rgb, img0_rgb)\n",
    "# print(temp_stitch.shape)\n",
    "final_image = image_stitch(img2_rgb, temp_stitch)\n",
    "plt.imshow(final_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shape the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_shape(image):\n",
    "    image_shape = image.shape\n",
    "    for i in range(image.shape[1]-1, 0, -1):\n",
    "        if np.sum(image[:, i, :]) != 0:\n",
    "            break\n",
    "    \n",
    "    image_reshape = Image.fromarray(image, mode=\"RGB\")\n",
    "\n",
    "    image_reshape = image_reshape.crop((0, 0, i+1, image.shape[0]))\n",
    "\n",
    "    return image_reshape\n",
    "\n",
    "image_stitched = image_shape(final_image)\n",
    "\n",
    "image_output_path = Path(output_path/images_references[24])\n",
    "print(images_references[24])\n",
    "\n",
    "image_stitched.save(image_output_path, quality=100)\n",
    "plt.imshow(image_stitched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tqdm, tqdm.notebook\n",
    "\n",
    "tqdm.tqdm = tqdm.notebook.tqdm  # notebook-friendly progress bars\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from hloc import (\n",
    "    extract_features,\n",
    "    match_features,\n",
    "    reconstruction,\n",
    "    visualization,\n",
    "    pairs_from_exhaustive,\n",
    ")\n",
    "from hloc.visualization import plot_images, read_image\n",
    "from hloc.utils import viz_3d\n",
    "import torch\n",
    "import pycolmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)\n",
    "print(\"torch.cuda.current_device()=\", torch.cuda.get_device_name())\n",
    "\n",
    "images = Path(\"/home/cv_stu_03/Hierarchical-Localization/project/images/MainLibrary/ReconstructionDataset\")\n",
    "sfm_dir= Path(\"/home/cv_stu_03/Hierarchical-Localization/project/Comparison/disk_disk+lightglue/sfm\")\n",
    "sfm_pairs= Path(\"/home/cv_stu_03/Hierarchical-Localization/project/Comparison/disk_disk+lightglue/pairs-sfm.txt\")\n",
    "features= Path(\"/home/cv_stu_03/Hierarchical-Localization/project/Comparison/disk_disk+lightglue/features.h5\")\n",
    "matches= Path(\"/home/cv_stu_03/Hierarchical-Localization/project/Comparison/disk_disk+lightglue/matches.h5\")\n",
    "references = [p.relative_to(images).as_posix() for p in (images ).iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pycolmap.Reconstruction(str(sfm_dir))\n",
    "\n",
    "fig = viz_3d.init_figure()\n",
    "viz_3d.plot_reconstruction(\n",
    "    fig, model, color=\"rgba(255,0,0,0.5)\", name=\"mapping\", points_rgb=True\n",
    ")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_conf = extract_features.confs[\"disk\"]\n",
    "matcher_conf = match_features.confs[\"disk+lightglue\"]\n",
    "loc_pairs = Path(\"/home/cv_stu_03/Hierarchical-Localization/project/pairs-from-loc.txt\")\n",
    "extract_features.main(\n",
    "    feature_conf, images_path, image_list=images_references, feature_path=features, overwrite=True\n",
    ")\n",
    "pairs_from_exhaustive.main(loc_pairs, image_list=images_references, ref_list=references)\n",
    "match_features.main(\n",
    "    matcher_conf, loc_pairs, features=features, matches=matches, overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitch_references = sorted([p.relative_to(output_path).as_posix() for p in (output_path).iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycolmap\n",
    "from hloc.localize_sfm import QueryLocalizer, pose_from_cluster\n",
    "\n",
    "cameras = []\n",
    "rets = []\n",
    "logs = []\n",
    "\n",
    "for i in range(len(stitch_references)):\n",
    "    # print(image_path/image_reference[i])\n",
    "\n",
    "    camera = pycolmap.infer_camera_from_image(output_path / stitch_references[i])\n",
    "    ref_ids = [model.find_image_with_name(r).image_id for r in references]\n",
    "    conf = {\n",
    "        \"estimation\": {\"ransac\": {\"max_error\": 12}},\n",
    "        \"refinement\": {\"refine_focal_length\": True, \"refine_extra_params\": True},\n",
    "    }\n",
    "    localizer = QueryLocalizer(model, conf)\n",
    "    # print(str(image_path/image_reference[i]))\n",
    "    ret, log = pose_from_cluster(localizer, str(stitch_references[i]), camera, ref_ids, features, matches)\n",
    "    cameras.append(camera)\n",
    "    rets.append(ret)\n",
    "    logs.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = pycolmap.Image(cam_from_world=rets[0][\"cam_from_world\"])\n",
    "viz_3d.plot_camera_colmap(\n",
    "    fig, pose, cameras[0], color=\"rgba(0,255,0,0.5)\", name=stitch_references[0], fill=True\n",
    ")\n",
    "# visualize 2D-3D correspodences\n",
    "inl_3d = np.array(\n",
    "    [model.points3D[pid].xyz for pid in np.array(logs[0][\"points3D_ids\"])[rets[0][\"inliers\"]]]\n",
    ")\n",
    "viz_3d.plot_points(fig, inl_3d, color=\"lime\", ps=1, name=stitch_references[0])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
